<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Foundations of Agentic Systems Theory</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<style>
		    .past {
		      color: #CCC;
		    }
		    .past a {
		      color: #CCC;
		      text-decoration: none;
		    }
		    .past a:hover {
		      color: #777;
		    }
		  </style>

		  <script>
			  document.addEventListener("DOMContentLoaded", () => {
			    const nowAoE = new Date(Date.now() - 12 * 60 * 60 * 1000);

			    const rows = document.querySelectorAll("#schedule tr");
			    rows.forEach(row => {
			      const dateCell = row.querySelector("td strong");
			      if (dateCell) {
			        const cleanText = dateCell.textContent.replace("(AoE)", "").trim();
			        const parsedDate = new Date(cleanText + " 23:59:59 UTC");
			        if (parsedDate < nowAoE) {
			          row.classList.add("past");
			        }
			      }
			    });

			    const submissionDeadline = new Date("October 25, 2025 23:59:59 UTC");
			    const submitButton = document.querySelector('a.button.primary[href*="openreview.net"]');

			    if (submitButton && nowAoE > submissionDeadline) {
			      submitButton.style.backgroundColor = "#aaa";
			      submitButton.style.borderColor = "#aaa";
			      submitButton.style.cursor = "not-allowed";
			      submitButton.style.pointerEvents = "none";
			      submitButton.style.textDecoration = "line-through";
			      submitButton.textContent = "Submit Paper";
			    }
			  });
			</script>

	</head>

	<body class="is-preload">

		<div id="wrapper">

			<!-- <header id="header">
				<div class="inner">

					<nav>
						<ul>
							<li><a href="#menu">Menu</a></li>
						</ul>
					</nav>

				</div>
			</header> -->

			<!-- <nav id="menu">
				<h2>FAST</h2>
				<ul>
					<li><a href="index.html">Home</a></li>
					<li><a href="generic.html">Ipsum veroeros</a></li>
					<li><a href="generic.html">Tempus etiam</a></li>
					<li><a href="generic.html">Consequat dolor</a></li>
					<li><a href="elements.html">Elements</a></li>
				</ul>
			</nav> -->

			<div id="main">
				<div class="inner">

					<div class="hero">
					  <div class="title-col">
					    <div class="title-group">
					      <h1 class="hero-title">
					        <span>Foundations of</span>
					        <span>Agentic</span>
					        <span>Systems</span>
					        <span>Theory</span>
					      </h1>
					    </div>

					    <div class="meta">
					      <p class="hero-subtitle">FAST @ <a href="https://aaai.org/conference/aaai/aaai-26/">AAAI 2026</a></p>
					      <p class="hero-subtitle">Jan 27, 2026</p>
					    </div>
					  </div>

					  <div class="hero-image">
					    <img src="images/fast.gif" alt="FAST">
					  </div>
					</div>

					<br>
					<br>

					<p>
						As with any complex system, the most interesting and consequential behaviors often arise not from the parts in isolation, but from the patterns of interaction between them. The current development of agentic AI has largely ignored these considerations, instead focusing on designing more (individually) capable agents. Failing to consider these effects as AI agents become more widespread will lead to a significant underestimation in both their capabilities and risks.
					</p>

					<p>
						There is an extensive body of knowledge underlying these interaction effects across various fields, but it‚Äôs not currently clear how applicable existing theoretical tools are to agentic AI systems. Tools from control theory and game/economic theory typically impose strong structural assumptions on both agents and the overall system (such as the form of objective functions, state evolution/dynamics, or degree of rationality) in efforts to obtain concrete results. On the other hand, methods from the social sciences use observations of human behavior, cultural contexts, and social norms to make more measured claims about probable patterns within the complexity and variability of human experience. Agentic AI systems don‚Äôt cleanly map to either of these settings. The underlying LLM in an AI agent does not possess the same rational behavior as idealized control/game/economic agents, nor does it exhibit the culturally/emotionally/evolutionarily shaped behaviors that characterize human agents.
					</p>

					<p>
						The <strong>Foundations of Agentic Systems Theory (FAST)</strong> workshop broadly aims to help evaluate the degree to which existing theory can be used to describe the behavior of agentic AI systems. Drawing from a variety of fields (notably beyond computer science, including developmental psychology, neuroscience, and social dynamics), FAST will explore both if and how existing mechanisms of emergent behavior from other systems carry over to systems of LLM-based agents, the properties of the underlying agents (and their LLMs) that facilitate these behaviors, and our ability to control/induce desirable system-wide outcomes. We strongly seek interdisciplinary participation (via both contributions and invited talks), with the ultimate goal of fundamentally contributing to a better understanding of the underlying processes that govern the system-level behavior (and risks) of agentic AI.
					</p>


					<br>

					<ul class="actions">
						<li><a href="https://openreview.net/group?id=AAAI.org/2026/Workshop/FAST" class="button primary">Submit Paper</a></li>
						<li><a href="https://aaai.org/conference/aaai/aaai-26/registration/#ws-only" class="button primary">Registration</a></li>
						<li><a href="https://www.singaporeexpo.com.sg" class="button primary">Venue</a></li>
					</ul>

					<section>
						<h2>Invited Speakers</h2>
						
						<p>We are pleased to have the following keynote speakers as part of the FAST program.</p>

						<div class="speakers-grid">

							<article class="speaker">
								<a class="avatar" href="https://www.cs.ox.ac.uk/people/michael.wooldridge/">
								  <img src="images/michael_wooldridge.jpg" alt="Portrait of Michael Wooldridge" loading="lazy" width="88" height="120">
								</a>

								<div class="info">
								  <h3 class="name">
								    <a href="https://www.cs.ox.ac.uk/people/michael.wooldridge/">Michael Wooldridge</a>
								  </h3>
								  <p class="affiliation">Faculty @<br> University of Oxford /<br>Hertford College</p>
								</div>

								<div class="talk-block">
									<div class="abstract">
										<details class="abstract">
    										<summary>Biography</summary>
    										<div class="abstract-content">
												Dr. Michael Wooldridge is the Ashall Professor of the Foundations of Artificial Intelligence at the University of Oxford and a Senior Research Fellow at Hertford College. Formerly Head of Oxford‚Äôs Computer Science Department (2014‚Äì21) and a Professor at the University of Liverpool, he is a Fellow of ACM, AAAI, EURAI, AISB, BCS, and Academia Europaea. His awards include the Lovelace Medal (2020), AAAI/EAAI Outstanding Educator Award (2021), and the EurAI Distinguished Service Award (2023). He has held major leadership roles in IJCAI, EurAI, and IFAAMAS, and co-edits the Artificial Intelligence Journal. Author of "The Road to Conscious Machines" and "A Brief History of AI", he also delivered the 2023 Royal Institution Christmas Lectures broadcast on BBC TV.
											</div>
  										</details>
									</div>
								</div>
							</article>

							<article class="speaker">
								<a class="avatar" href="https://ey242.github.io/about.html">
								  <img src="images/eunice_yiu.png" alt="Portrait of Eunice Yiu" loading="lazy" width="88" height="120">
								</a>

								<div class="info">
								  <h3 class="name">
								    <a href="https://ey242.github.io/about.html">Eunice Yiu</a>
								  </h3>
								  <p class="affiliation">Postdoctoral Scholar @<br>UC Berkeley</p>
								</div>

								<div class="talk-block">
									<div class="abstract">
										<details class="abstract">
    										<summary>Biography</summary>
    										<div class="abstract-content">
												Dr. Eunice Yiu studies how children generalize from few observations, using abstraction, analogy, and active exploration to build causal models of the world. By comparing these capacities with AI systems, she develops developmental datasets and curricula that reveal the strengths and limits of AI models, and design human-inspired approaches for building more adaptive and flexible AI. Her work is in collaboration with <a href="https://bair.berkeley.edu/">Berkeley AI Research Lab</a> and <a href="https://deepmind.google/">Google DeepMind</a>. Before starting her PhD, she received a BA in Psychology, Biology and Economics from Cornell University.
											</div>
  										</details>
									</div>
								</div>
							</article>

							<article class="speaker">
								<a class="avatar" href="https://sara-fish.github.io">
								  <img src="images/sara_fish.jpg" alt="Portrait of Sara Fish" loading="lazy" width="88" height="120">
								</a>

								<div class="info">
								  <h3 class="name">
								    <a href="https://sara-fish.github.io">Sara Fish</a>
								  </h3>
								  <p class="affiliation">PhD Student @<br>Harvard University</p>
								</div>

								<div class="talk-block">
									<div class="abstract">
										<details class="abstract">
    										<summary>Biography</summary>
    										<div class="abstract-content">
												Sara Fish is a fourth-year PhD student at Harvard advised by <a href="https://yannai.gonch.name/scientific/">Yannai Gonczarowski</a>. Her research lies at the intersection of Economics and Computer Science (EconCS) and machine learning, with recent work exploring how AI systems behave in economic settings. Her contributions include "Algorithmic Collusion by Large Language Models" which investigates anti-competitive behavior in LLM-based agents, and developing benchmarks for evaluating LLM agents in economic environments. Her work on generative social choice theory, studying how AI can aggregate diverse human preferences, earned a $100,000 OpenAI Democratic Inputs grant.
											</div>
  										</details>
									</div>
								</div>
							</article>

						</div>

					</section>

					<section>
						<header class="major" id="program">
							<h2>Schedule</h2>
						</header>

						<p>All times are in Singapore Time (SGT, UTC+8).</p>

						<table class="schedule-table">
							<thead>
								<tr>
									<th>Time</th>
									<th>Session</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td class="time-col">09:00 ‚Äì 09:10</td>
									<td><strong>Opening Remarks</strong></td>
								</tr>
								<tr>
									<td class="time-col">09:10 ‚Äì 09:50</td>
									<td>
									    <strong>Keynote: Michael Wooldridge</strong>
									    <ul class="paper-list">
									      <li>
									        <span class="paper-title">
									          Rethinking Multi-Agent Systems in the Era of LLMs
									        </span>
									      </li>
									    </ul>
									</td>
								</tr>
								<tr>
									<td class="time-col">09:50 ‚Äì 10:30</td>
									<td>
									    <strong>Keynote: Eunice Yiu</strong>
									    <ul class="paper-list">
									      <li>
									        <span class="paper-title">
									          Empowerment as a Foundation for Agentic World Model Building:
									          From Children to Large Pretrained Models
									        </span>
									      </li>
									    </ul>
									</td>
								</tr>
								<tr class="break-row">
									<td class="time-col">10:30 ‚Äì 11:00</td>
									<td>Coffee Break</td>
								</tr>
								<tr>
									<td class="time-col">11:00 ‚Äì 11:30</td>
									<td>
										<strong>Contributed Talks I</strong>
										<ul class="paper-list">
											<li>
												<span class="paper-title">Does Self-Evaluation Enable Wireheading in Language Models?</span>
												<span class="paper-authors">David Demitri Africa, Hans Ethan Keh Ting</span>
											</li>
											<li>
												<span class="paper-title">Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation</span>
												<span class="paper-authors">Zairah Mustahsan, Abel Lim, Megna Anand, Saahil Jain, Bryan McCann</span>
											</li>
											<li>
												<span class="paper-title">Formalizing Observability in Agentic AI Systems</span>
												<span class="paper-authors">Daniele Lotito, Massimiliano Pronesti</span>
											</li>
										</ul>
									</td>
								</tr>
								<tr>
									<td class="time-col">11:30 ‚Äì 12:15</td>
									<td><strong>Poster Session I</strong></td>
								</tr>
								<tr class="break-row">
									<td class="time-col">12:15 ‚Äì 13:30</td>
									<td>Lunch Break</td>
								</tr>
								<tr>
									<td class="time-col">13:30 ‚Äì 14:15</td>
									<td>
									    <strong>Keynote: Sara Fish</strong>
									    <ul class="paper-list">
									      <li>
									        <span class="paper-title">
									          Algorithmic Collusion by Large Language Models
									        </span>
									      </li>
									    </ul>
									</td>
								</tr>
								<tr>
									<td class="time-col">14:15 ‚Äì 14:45</td>
									<td>
										<strong>Contributed Talks II</strong>
										<ul class="paper-list">
											<li>
												<span class="paper-title">The Multi-Agent Off-Switch Game</span>
												<span class="paper-authors">Akash Agrawal, Soroush Ebadian, Lewis Hammond</span>
											</li>
											<li>
												<span class="paper-title">The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems</span>
												<span class="paper-authors">Robert Yang</span>
											</li>
											<li>
												<span class="paper-title">R<sub>0</sub> for Agentic Tool-Networks: Spectral Thresholds and Intervention Levers in LLM-Agent Systems</span>
												<span class="paper-authors">Aviral Srivastava, Sourav Panda, Kushagra Srivastva</span>
											</li>
										</ul>
									</td>
								</tr>
								<tr>
									<td class="time-col">14:45 ‚Äì 15:30</td>
									<td><strong>Poster Session II</strong></td>
								</tr>
								<tr class="break-row">
									<td class="time-col">15:30 ‚Äì 16:00</td>
									<td>Coffee Break</td>
								</tr>
								<tr>
									<td class="time-col">16:00 ‚Äì 17:00</td>
									<td><strong>Round Table Discussion and Closing Remarks</strong></td>
								</tr>
							</tbody>
						</table>
					</section>

					<section>		
						<header class="major" id="dates">
							<h2>Important Dates</h2>
						</header>
								
					  	<table id="schedule">
					    	<tbody>
								<tr><td><strong>September 12, 2025</strong></td><td>Submission window opens (<a href="https://openreview.net/group?id=AAAI.org/2026/Workshop/FAST">OpenReview</a>)</td></tr>
								<!-- <tr>
								  <td>
								    <strong style="text-decoration: line-through; text-decoration-color: red;">October 19, 2025</strong>
								    <strong style="color: red;">October 22, 2025</strong>
								  </td>
								  <td>Abstract submission deadline</td>
								</tr>
								<tr> -->
								  <td>
								    <strong>October 25, 2025</strong> (AoE)
								  </td>
								  <td>Paper submission deadline</td>
								</tr>
								<tr><td><strong>November 8, 2025 (AoE)</strong></td><td>Acceptance notification</td></tr>
								<tr><td><strong>November 19, 2025</strong></td><td>Early registration ends</td></tr>
								<tr><td><strong>December 14, 2025</strong></td><td>Refund deadline; late registration ends</td></tr>
								<tr><td><strong>January 27, 2026</strong></td><td>Workshop</td></tr>
						    </tbody>
				  		</table>
			  		</section>

					<section>
						<h2>Scope and Topics</h2>
						<p>
							Large language models have recently become sophisticated enough to be reliably integrated into more complex pipelines, leading to more automated (i.e., agentic) use cases. However, the community has focused disproportionately on <em>building</em> these systems rather than <em>understanding</em> why they may (or may not) work. The goal of the FAST workshop is to investigate how both existing theory (notably that outside of the traditional AI community) and new insights (unique to LLM-based agents) can help to build this understanding.
						</p>

						<p>
							As such, we invite submissions on the following topics:
							<ul>
								<li>Mechanisms of emergent capabilities (in both biological and artificial agents)</li>
								<li>Evaluation, detection, and mitigation/bounding of emergent capabilities in AI systems</li>
								<li>Incentive mechanisms for inducing behavior in systems of LLM-based agents</li>
								<li>Definitions of agency (of agents, and of systems); philosophy of agency in engineered systems</li>
								<li>Definitions of emergence in engineered systems</li>
								<li>Benchmarks and datasets for monitoring capabilities/agency, risks, and failure modes in agentic AI systems</li>
							</ul>
						</p>
					</section>


					<section>
					    <header class="major" id="accepted_papers">
					        <h2>Accepted Papers</h2>
					    </header>

					    <ul class="accepted-papers-list">
					        <li>
					            <span class="paper-title">A Coherence-Based Measure of AGI</span>
					            <span class="paper-authors">Fares Fourati</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Recent work by Hendrycks et al. (2025) formalized Artificial General Intelligence (AGI) as the arithmetic mean of proficiencies across cognitive domains derived from the Cattell-Horn-Carroll (CHC) model of human cognition. While elegant, this definition assumes compensability‚Äîthat exceptional ability in some domains can offset failure in others. True general intelligence, however, should reflect coherent sufficiency: balanced competence across all essential domains. We propose a coherence-aware measure of AGI based on the integral of generalized means over a continuum of compensability exponents. This formulation spans arithmetic, geometric, and harmonic regimes, and the resulting area under the curve (AUC) quantifies robustness under varying compensability assumptions. Unlike the arithmetic mean, which rewards specialization, the AUC penalizes imbalance and captures inter-domain dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5, the coherence-adjusted AUC reveals that both systems remain far from general competence despite high arithmetic scores (e.g., GPT-5 at ~24%). Integrating the generalized mean thus yields a principled, interpretable, and stricter foundation for measuring genuine progress toward AGI.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">A Theoretical Framework for Measuring Organisational Decentralisation in Agentic Design</span>
					            <span class="paper-authors">Andrea Marino, Giovanni Sileno, Thomas van Binsbergen, Tom van Engers</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Contemporary Agentic AI systems face challenges as lack of traceability, semantic drift, and frictions arising from inter-agent misalignment. In this paper, we present an approach to Agentic AI that addresses these issues by abstracting the problem of governance to organisational templating. Our methodology proposes to formalise agentic design based on established sociological perspectives and business process management practices. Additionally, we connect it to supply-demand concepts in order to introduce a measure of decentralisation, and we hypothesise that this metric can be used to guide the design and prevent responsibility gaps to form during its inception. We conclude by outlining an experimental setup that leverages all the concepts introduced.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Constrained Process Maps for Multi-Agent Generative AI Workflows</span>
					            <span class="paper-authors">Ananya Joshi, Michael Rudow</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Large language model (LLM)‚Äìbased agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. Yet, many agentic architectures focus on prompt engineering for single agents, which makes it difficult to observe or compare how models are considering uncertainty and coordination across interconnected decision stages and with humans. This paper introduces a multi-agent system design formalized as a bounded-horizon, directed, acyclic Markov Decision Process (MDP). Each agent in this system corresponds to a specific step or role (e.g., content, business, legal in a compliance setting), with set transitions between agents representing task escalation or completion. Epistemic uncertainty (per agent) is quantified using Monte Carlo estimation, and system-level uncertainty (across agents) is characterized the MDP setup terminating in a labeled state or one with human review. We illustrate the approach with a case study in AI safety evaluation for self-harm detection via a multi-agent compliance system based on this set-up. Results show improvements over a single-agent baseline in accuracy (up to 19%), reduction in required human review (up to 85√ó), and, in some configurations, less processing time.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Does Self-Evaluation Enable Wireheading in Language Models?</span>
					            <span class="paper-authors">David Demitri Africa, Hans Ethan Keh Ting</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Federated Agent Reinforcement Learning</span>
					            <span class="paper-authors">Canyu Chen, Kangyu Zhu, Zhaorun Chen, Zhanhui Zhou, Shizhe Diao, Yiping Lu, Tian Li, Manling Li, Dawn Song</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Autonomous AI Agents powered by LLMs have shown remarkable abilities in diverse domains. However, the training process typically require centralized collection of large amounts of real-world user data, posing substantial privacy and regulatory concerns. To this end, we explore a new decentralized training paradigm, namely FedAgent (Federated Agent Reinforcement Learning), which enables collaborative learning of AI agents across distributed clients without sharing local data. Moreover, we construct the first decentralized agent learning environment FedAgentGym, which includes four types of LLM agents, two application scenarios (WebShop and ALFWorld), three variations of decentralized settings, and three newly defined heterogeneity challenges (Preference Heterogeneity, Coverage Heterogeneity, and Hardness Heterogeneity), to systematically investigate its effectiveness and impact factors. Extensive theoretical and empirical studies show that FedAgent can have comparable performance to the centralized training paradigm and exhibit strong robustness against heterogeneities, which shows the feasibility of training AI agents without sacrificing data privacy. The code is available.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Formalizing Observability in Agentic AI Systems</span>
					            <span class="paper-authors">Daniele Lotito, Massimiliano Pronesti</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    A system can be more than the sum of its parts. Agentic systems are central to AI development because they exhibit emergent capabilities that cannot be inferred from studying individual agents alone. However, these systems are challenging to analyze: components such as agents, the LLMs powering them, and their associated tools often function as black boxes. Moreover, the diversity of implementations makes a universal approach to characterizing network properties impractical. We propose that initial studies of emergent behavior in agentic systems should focus on systems where each agent initiates the action of at most one other agent. To support this, we present a theoretical model of agentic systems that emphasizes the role of observability layers in monitoring both agent‚Äìagent and agent‚Äìenvironment interfaces. We further discuss how these layers facilitate the study of system-level behavior and constitute a fundamental component in the design of agentic AI systems.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">From Agentic AI to Autonomous Agents</span>
					            <span class="paper-authors">Shiwali Mohan</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Agentic AI has reenergized the research on intelligent agents and autonomy. In this paper, I revisit the computational theory of intelligent agency that has emerged from scientific consensus in multiple sub-disciplines of AI research. I discuss how modern foundation models map to that theory and propose how agentic AI systems can be extended to become autonomous agents.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">From Object to Other: A Practical Theory of AI Moral Status and Personhood in Re-evaluating AI Safety Methods</span>
					            <span class="paper-authors">Vaishnavi Singh, Stephanie Choi, Desiree Junfijiah</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    This paper offers practical guidance on AI welfare in industry. While previous scholarship has centered on the theoretical nuances of the definition of moral status or the definition and validity of the many properties that may constitute it, little attention has been given to the practical implementation of such work. Within this paper, we introduce a framework that classifies AI systems along two axes‚Äîevidence of personhood and observed controllability‚Äîyielding four operational classes (A-D: controllable & lacking moral status, controllable & possessing moral status, uncontrollable & possessing moral status, uncontrollable & lacking moral status) and three tiers of moral status: Tier 0 (Presumed Object), Tier 1 (Ambiguous Other), and Tier 2 (Confirmed Other). We first develop an industry-applicable, practical theory of indicators that an AI system must satisfy to attain any moral status (Tier 1) or to qualify for moral personhood (Tier 2), organized into four criteria: consciousness, theory of mind, self-awareness, and robust agency. Next, we argue that pre-existing AI safety evaluations can function as dual-use assessments that simultaneously test AI safety metrics and probe for our aforementioned moral status indicators from our practical theory, and we detail how specific components of alignment techniques can serve this dual function. Lastly, we propose co-alignment for AI entities belonging in Class B. We do not take a stance on whether or not any present system is conscious or will be, but argue that the combination of the non-negligible chance of AI deserving of moral consideration as well as the moral significance of a false negative necessitate immediate preparation, regardless of timeline uncertainty.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Leapsight: Towards a Functional Account of Mediation Between Perception and Action</span>
					            <span class="paper-authors">Mateusz Bagi≈Ñski, Tushita Jha</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    This paper develops a functional account of agency that is not tied to fixed goals, utility functions, or specific computational architectures. Rather than defining agents by their internal structure‚Äîplans, predictors, policies‚Äîwe focus on what they do: maintain a workable coupling between their internal representations and the world they act within. We introduce Leapsight as a name for this teleological tendency toward sustained coordination between the system's internal states and the evolving environment. A system exhibits Leapsight when its behavior systematically drives this coordination‚Äîsometimes by adjusting its representations, sometimes by altering the world. This perspective shifts attention from structural descriptions of agents to the functional regularities that explain why both biological organisms and contemporary AI systems display influence-seeking, adaptive behavior, and persistence across changing conditions. Leapsight thus offers a lens for understanding agency as an emergent pattern of self-maintaining dynamics, rather than a property defined by specific mechanisms or goal-encoding formalisms.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">LENS: Learning Architecture Navigator for LLM Agentic Systems</span>
					            <span class="paper-authors">Guancheng Wan, Jiayi Yang, Mengting Li, Eric Hanchen Jiang, Haixin Wang, Hui Yi Leong, Yizhou Sun, Wei Wang</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration, while constructing these systems often requires labor-intensive manual designs. A frontier effort to automate this process is to optimize an Agentic Supernet, a probabilistic distribution of architectures from which query-dependent workflows can be dynamically sampled. However, while this paradigm allows for dynamic resource allocation, its underlying optimization process presents a critical performance bottleneck: inconsistent architectural feedback suppresses reliable credit assignment and prematurely narrows exploration, missing innovative and efficient designs. To address this, we introduce LENS (Learning-Enhanced Neural Search for Agentic Workflows), a dual-module framework that systematically resolves both challenges. The Adaptive Diversity Module (ADM) maintains comprehensive exploration across the architectural space, while the Retrospective Guidance Module (RGM) learns from historical evaluations to provide stable search direction. By decoupling diversity maintenance from directional guidance, LENS achieves robust search that discovers higher-utility, lower-cost configurations. Comprehensive evaluations across diverse benchmarks demonstrate that LENS is: (I) higher-performing, achieving up to 13.63% accuracy improvement on challenging benchmarks with the same search budget; (II) more sample-efficient, requiring only 30 training samples to outperform baselines trained on much larger datasets; and (III) more cost-effective, reducing inference token consumption by 7.8% while significantly improving performance.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length</span>
					            <span class="paper-authors">Chupei Wang, Jiaqiu Vince Sun</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams co-referenced key‚Äìvalue updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as co-referenced interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. For Agentic systems, reliable operation hinges on reconciling past and present states. Proactive interference corrupts long-horizon state maintenance; dependable agent tracking therefore needs explicit memory control and interference-aware context management. We expose a "know, cannot do" failure. Coreference-only needles expose a plan‚Äìexecute disjunction: LLMs form the correct last-value retrieval plan but fail to carry it out, with execution reliability declining systematically with task complexity. Code and data will be publicly available.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">R<sub>0</sub> for Agentic Tool-Networks: Spectral Thresholds and Intervention Levers in LLM-Agent Systems</span>
					            <span class="paper-authors">Aviral Srivastava, Sourav Panda, Kushagra Srivastva</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Agentic AI systems are increasingly built as networks of LLM agents connected by tools, memories, and communication channels. We introduce a practical diagnostic for self-propagation in these systems: the capability-weighted spectral radius of the tool graph, which in our framework plays the role of an effective reproduction number, R<sub>0</sub> = Œª<sub>max</sub>(ùíú). We evaluate the approach on synthetic, heterogeneous tool graphs (Erd≈ës-R√©nyi, scale-free, and two-tier DAG) using Monte Carlo simulation. Across topologies, a larger capability-weighted spectral radius consistently corresponds to larger activation cascades and lower extinction probability, with a clear subcritical/supercritical divide near Œª<sub>max</sub> ‚âà 1. A first-generation reproduction estimate increases monotonically with Œª<sub>max</sub>(ùíú), supporting the control-parameter interpretation. We also examine two common design levers used in agentic stacks: typed tools (narrower, schema-validated interfaces) and taint-aware memory caps (limits on self-referential content). At a fixed workload setting, tightening either lever moves systems left and down in the spectral-radius-vs.-outbreak plane, pushing them subcritical. Together, these results provide a diagnostic framework that is simple to compute, predictive across wiring patterns, and aligned with practical engineering controls for keeping R<sub>0</sub> < 1.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Relational Archetypes: A Comparative Analysis of AV-Human and Agent-Human Interactions</span>
					            <span class="paper-authors">Toni Lorente, Amin Oueslati, Robin Staes-Polet</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Over the last couple of years, AI Agents have gained significant traction due to substantial progress in the capabilities of underlying General Purpose AI (GPAI) models, enhanced scaffolding techniques, and the promise to drive societal transformation. Companies, researchers, and policy makers have started to consider the different effects that AI agents may have across different dimensions of our lives. However, the literature exploring the broader effects of human-agent interactions is still underdeveloped. In this paper, we review the problem of traffic modulation by autonomous vehicles (AVs) in mixed traffic flows and extrapolate the learnings to the different modes of interaction between humans and AVs to the pair humans-AI agents. In doing so, we propose a preliminary taxonomy of relational archetypes based on literature on Human-Computer Interaction (HCI) and AV-human interaction, and tentatively explore how the resulting framework may lead to new questions regarding human-agent interactions. Our effort is aimed at strengthening existing bridges between these two research communities, which share similar traits: autonomy, fast adoption, high impact, and great potential for economic transformation. Building on previous analogies between AI Agents and AVs (e.g., regarding autonomy levels), we anticipate this paper to spark scholarly debate on the different types of impact that agents may have on our societies, while inviting other researchers to expand the scope of their comparative analysis regarding AI Agents.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Sequential Causal Normal Form Games: Theory, Computation, and Strategic Signaling</span>
					            <span class="paper-authors">Dennis Thumm</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Can classical game-theoretic frameworks be extended to capture the bounded rationality and causal reasoning of AI agents? We investigate this question by extending Causal Normal Form Games (CNFGs) to sequential settings, introducing Sequential Causal Multi-Agent Systems (S-CMAS) that incorporate Pearl's Causal Hierarchy across leader-follower interactions. While theoretically elegant‚Äîwe prove PSPACE-completeness, develop equilibrium refinements, and establish connections to signaling theory‚Äîour comprehensive empirical investigation reveals a critical limitation: S-CNE provides zero welfare improvement over classical Stackelberg equilibrium across all tested scenarios. Through 50+ Monte Carlo simulations and hand-crafted synthetic examples, we demonstrate that backward induction with rational best-response eliminates any strategic advantage from causal layer distinctions. We construct a theoretical example illustrating conditions where benefits could emerge (Œµ-rational satisficing followers), though implementation confirms that even relaxed rationality assumptions prove insufficient when good instincts align with optimal play. This negative result provides valuable insight: classical game-theoretic extensions grounded in rational choice are fundamentally incompatible with causal reasoning advantages, motivating new theoretical frameworks beyond standard Nash equilibrium for agentic AI.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation</span>
					            <span class="paper-authors">Zairah Mustahsan, Abel Lim, Megna Anand, Saahil Jain, Bryan McCann</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run‚Äîobscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), revealing whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1‚Äì3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955‚Äì0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8‚Äì16 trials for structured tasks and for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to principled experimental science.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">The Multi-Agent Off-Switch Game</span>
					            <span class="paper-authors">Akash Agrawal, Soroush Ebadian, Lewis Hammond</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    The off-switch game framework has been instrumental in understanding corrigibility‚Äîthe property that AI agents should allow human oversight and intervention. In single-agent settings, uncertainty about human preferences naturally incentivizes agents to defer to human judgment. However, as AI systems increasingly operate in multi-agent environments, a crucial question arises: does corrigibility compose across multiple agents? We introduce the multi-agent off-switch game and demonstrate that individually corrigible agents can become collectively incorrigible when strategic interactions are considered. Through formal analysis and illustrative examples, we show that corrigibility is not compositional and identify conditions under which group incorrigibility emerges. Our results highlight fundamental challenges for AI safety in multi-agent settings and suggest the need for new approaches that explicitly address collective dynamics.
					                </div>
					            </details>
					        </li>
					        <li>
					            <span class="paper-title">The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems</span>
					            <span class="paper-authors">Robert Yang</span>
					            <details class="abstract">
					                <summary>Abstract</summary>
					                <div class="abstract-content">
					                    Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce the Akrasia Benchmark, a structured set of prompting conditions (Baseline B, Synonym S, Temporal T, and Temptation X) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.
					                </div>
					            </details>
					        </li>
					    </ul>
					</section>

					<!-- <section>
						<header class="major" id="submission">
							<h2>Submission Information</h2>
						</header>

						<p>
							Submissions can be either full or short papers:

							<ul>
								<li><strong>Full papers</strong>:
									Up to 7 pages (excluding references and appendices); should present mature or completed research. 
								</li>
								<li><strong>Short papers</strong>:
									Up to 4 pages (excluding references and appendices); intended for describing ongoing work, early-stage ideas, or the release of benchmarks and datasets (authors are encouraged to use the short paper format for benchmarks and datasets).
								</li>
							</ul>
						</p>

						<p>
							All submissions must be made through our <a href="https://openreview.net/group?id=AAAI.org/2026/Workshop/FAST">OpenReview page</a>. Please follow the <a href="https://aaai.org/conference/aaai/aaai-26/submission-instructions/">AAAI template</a> when preparing your submission.
						</p>

						<p>
							Submissions must be anonymized for double-blind review. Reviewing will follow the standards of AAAI, with evaluation based on novelty, technical depth, clarity, reproducibility, and potential impact. Accepted papers will be presented as either posters or lightning talks. At least one author of each accepted paper must register and attend the workshop. If you have any questions, please contact us at <a href="mailto:fast.workshop.team@gmail.com">fast.workshop.team@gmail.com</a>.
						</p>

					</section> -->

					<section> 
					
						<header class="major" id="chairs">
							<h2>Organizers</h2>
						</header>

						<div class="people-grid">

							<!-- Erik -->
							<article class="person">
								<a class="avatar" href="https://research.ibm.com/people/erik-miehling">
								  <img src="images/erik_miehling.jpeg" alt="Portrait of Erik Miehling">
								</a>
								<div class="info">
								  <h3 class="name">
								    <a href="https://research.ibm.com/people/erik-miehling" rel="noopener">Erik Miehling</a>
								  </h3>
								  <p class="affiliation">IBM Research - Ireland</p>
								  <p class="email"><a href="mailto:erik.miehling@ibm.com">erik.miehling@ibm.com</a></p>
								</div>
							</article>

							<!-- Chenchen -->
							<article class="person">
								<a class="avatar" href="https://yecchen.github.io/">
								  <img src="images/chenchen_ye.jpeg" alt="Portrait of Chenchen Ye">
								</a>
								<div class="info">
								  <h3 class="name"><a href="https://yecchen.github.io/" rel="noopener">Chenchen Ye</a></h3>
								  <p class="affiliation">University of California, Los Angeles</p>
								  <p class="email"><a href="mailto:ccye@cs.ucla.edu">ccye@cs.ucla.edu</a></p>
								</div>
							</article>

							<!-- Atoosa -->
							<article class="person">
								<a class="avatar" href="http://kasirzadeh.org/">
								  <img src="images/atoosa_kasirzadeh.jpeg" alt="Portrait of Atoosa Kasirzadeh">
								</a>
								<div class="info">
								  <h3 class="name"><a href="http://kasirzadeh.org/" rel="noopener">Atoosa Kasirzadeh</a></h3>
								  <p class="affiliation">Carnegie Mellon University</p>
								  <p class="email"><a href="mailto:atoosa@cmu.edu">atoosa@cmu.edu</a></p>
								</div>
							</article>

							<!-- Djallel -->
							<article class="person">
								<a class="avatar" href="https://research.ibm.com/publications?author=1736">
								  <img src="images/djallel_bouneffouf.jpeg" alt="Portrait of Djallel Bouneffouf">
								</a>
								<div class="info">
								  <h3 class="name"><a href="https://research.ibm.com/publications?author=1736" rel="noopener">Djallel Bouneffouf</a></h3>
								  <p class="affiliation">IBM Research - Yorktown Heights</p>
								  <p class="email"><a href="mailto:djallel.bouneffouf@ibm.com">djallel.bouneffouf@ibm.com</a></p>
								</div>
							</article>

							<!-- Anne -->
							<article class="person">
								<a class="avatar" href="https://www.wis.ewi.tudelft.nl/arzberger">
								  <img src="images/anne_arzberger.jpeg" alt="Portrait of Anne Arzberger">
								</a>
								<div class="info">
								  <h3 class="name"><a href="https://www.wis.ewi.tudelft.nl/arzberger" rel="noopener">Anne Arzberger</a></h3>
								  <p class="affiliation">TU Delft</p>
								  <p class="email"><a href="mailto:a.arzberger@tudelft.nl">a.arzberger@tudelft.nl</a></p>
								</div>
							</article>

						</div>

					</section>

					<!--
					<section>
						<header class="major" id="accepted_papers">
							<h3>Accepted Papers</h3>
						</header>

						<h4 id="spotlight">Lightning Talks</h4>
						<ul>
							<li><font color="#">Title</font><br>
								Author (affiliation)
							</li>
						</ul>

						<h4>Poster Presentations</h4>
						<ul>
							<li><font color="#">Title</font><br>
								Author (affiliation)
							</li>
						</ul>
						
					</section>
					-->

					<br>

					<section>							
						<header class="major" id="pc_members"> 
							<h3>Program Committee</h3> 
						</header>

						<p>We are grateful to the following people for helping make the FAST workshop a success:</p>

						<ul>
							<li>Adam Dahlgren Lindstr√∂m (Ume√• University)</li>
							<li>Alex Zhang (PhD candidate, UIUC)</li>
							<li>Antonin Sulc (Berkeley Lab)</li>
							<li>Bjorn de Koning (Erasmus University Rotterdam)</li>
							<li>Daiki Kimura (IBM Japan)</li>
							<li>David Santandreu (MBZUAI)</li>
							<li>Dennis Wei (IBM Research)</li>
							<li>Dmitry Zubarev (IBM Research)</li>
							<li>Ekdeep Singh Lubana (Harvard)</li>
							<li>Emanuele Sansone (MIT)</li>
							<li>Emre Acart√ºrk (PhD candidate, RPI)</li>
							<li>Enrico Liscio (TU Delft)</li>
							<li>Hariram Veeramani (PhD candidate, UCLA)</li>
							<li>Ivoline Ngong (PhD candidate, University of Vermont)</li>
							<li>Jay Nanavati (IQVIA)</li>
							<li>Jinqi Luo (PhD student, University of Pennsylvania)</li>
							<li>Kartik Ahuja (FAIR, Meta)</li>
							<li>Konstantinos Roumeliotis (University of Peloponnese)</li>
							<li>Mariya Hendriksen (Microsoft Research)</li>
							<li>Mats Leon Richter (H Company)</li>
							<li>Penny Pexman (Western University)</li>
							<li>Peter Belcak (NVIDIA)</li>
							<li>Praveen Venkateswaran (IBM Research)</li>
							<li>Ranjan Sapkota (Cornell)</li>
							<li>Shengran Hu (PhD candidate, UBC)</li>
							<li>Saranya Vijayakumar (PhD candidate, CMU)</li>
							<li>Shubham Subhnil (PhD candidate, Trinity College)</li>
							<li>Srishti Yadav (PhD candidate, University of Copenhagen)</li>
							<li>Thorsten Hellert (Berkeley Lab)</li>
							<li>Tianwei Xing (UCLA)</li>
							<li>Tim Klinger (IBM Research)</li>
							<li>Victor Dibia (Microsoft Research)</li>
						</ul>

					</section>

					<!--
					<section>
						<header class="major" id="sponsors">
							<h2>Sponsors</h2> 
						</header>								
					</section>
					-->

				</div>

			</div>

			<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<section>
						<h2>Contact</h2>
						<ul class="icons">
							<li>
								<a href="mailto:fast.workshop.team@gmail.com">fast.workshop.team@gmail.com</a>
							</li>
						</ul>							
					</section>
					<ul class="copyright">
						<li>Design forked from: <a href="http://html5up.net">HTML5 UP</a></li>
						<li>Graphics courtesy of: <a href="https://www.canva.com">Canva</a></li>
					</ul>
				</div>
			</footer>

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>